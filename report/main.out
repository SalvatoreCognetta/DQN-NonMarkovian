\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Deterministic Finite Automata}{}% 2
\BOOKMARK [1][-]{section.3}{Proximal Policy Optimization}{}% 3
\BOOKMARK [1][-]{section.4}{ Markov Decision Process}{}% 4
\BOOKMARK [1][-]{section.5}{Non Markovian Rewards Decision Processes}{}% 5
\BOOKMARK [1][-]{section.6}{Gym-Sapientino}{}% 6
\BOOKMARK [2][-]{subsection.6.1}{Temporal Goal on Gym-Sapientino}{section.6}% 7
\BOOKMARK [1][-]{section.7}{The non markovian agent}{}% 8
\BOOKMARK [1][-]{section.8}{Implementation Details}{}% 9
\BOOKMARK [2][-]{subsection.8.1}{Proposed network implementation}{section.8}% 10
\BOOKMARK [2][-]{subsection.8.2}{Network Description}{section.8}% 11
\BOOKMARK [2][-]{subsection.8.3}{Reward shaping}{section.8}% 12
\BOOKMARK [1][-]{section.9}{Experimental environment}{}% 13
\BOOKMARK [1][-]{section.10}{Trials}{}% 14
\BOOKMARK [1][-]{section.11}{Experiment with two colors }{}% 15
\BOOKMARK [1][-]{section.12}{Experiments with two colors with bigger maps}{}% 16
\BOOKMARK [2][-]{subsection.12.1}{First Trial}{section.12}% 17
\BOOKMARK [2][-]{subsection.12.2}{Second Trial}{section.12}% 18
\BOOKMARK [1][-]{section.13}{Experiment with three colors}{}% 19
\BOOKMARK [1][-]{section.14}{Conclusion and result discussion}{}% 20
